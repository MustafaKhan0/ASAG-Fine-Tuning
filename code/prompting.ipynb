{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import jsonlines\n",
    "from pprint import pprint\n",
    "import tiktoken\n",
    "import openai\n",
    "import xmltodict as xmd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xml(path: Path, dataset: str):\n",
    "    '''\n",
    "    process_xml : Converts xml file from scientsbank or beetle dataset to a pandas DataFrame\n",
    "    ----------\n",
    "    Parameters\n",
    "    path : pathlib.Path\n",
    "        The path to the xml file which you want to convert\n",
    "    dataset : str\n",
    "        The dataset being processed - either \"scientsbank\" or \"beetle\" (otehrwise raises ValueError)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing the data from that xml file\n",
    "    '''\n",
    "\n",
    "    dataset = dataset.lower()\n",
    "\n",
    "    if dataset not in ['scientsbank', 'beetle']:\n",
    "        raise ValueError(f'\\\"{dataset}\\\" is not a valid value for \\'dataset\\' input. Use either \\\"sciEntsBank\\\" or \\\"beetle\\\".')\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.read()\n",
    "    master = xmd.parse(data)\n",
    "\n",
    "    q_id = master['question']['@id']; q_text = master['question']['questionText']; q_module =  master['question']['@module']\n",
    "\n",
    "    \n",
    "    reference_answers = master['question']['referenceAnswers']['referenceAnswer']\n",
    "\n",
    "    \n",
    "    reference_processed = [None, None, None]\n",
    "    if dataset == 'scientsbank':\n",
    "        best_answers = [reference_answers]\n",
    "        good_answers = []\n",
    "        minimal_answers = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    elif type(reference_answers) == dict:\n",
    "        best_answers = [reference_answers if reference_answers['@category'] == 'BEST' else []]\n",
    "        good_answers = [reference_answers if reference_answers['@category'] == 'GOOD' else []]\n",
    "        minimal_answers = [reference_answers if reference_answers['@category'] == 'MINIMAL' else []]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    elif type(reference_answers) in [set, list]:\n",
    "        best_answers = [a for a in reference_answers if a['@category'] == 'BEST']\n",
    "        good_answers = [a for a in reference_answers if a['@category'] == 'GOOD']\n",
    "        minimal_answers = [a for a in reference_answers if a['@category'] == 'MINIMAL']\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "  \n",
    "    reference_processed = [best_answers, good_answers, minimal_answers]\n",
    "    answers = master['question']['studentAnswers']['studentAnswer']\n",
    "    data = []\n",
    "    for answer in answers:\n",
    "        data.append(\n",
    "            [q_id, q_text, q_module] + reference_processed + [answer['@id'], answer['#text'], answer['@accuracy']]\n",
    "        )\n",
    "    \n",
    "    columns = ['question_id', 'question_text', 'module', 'best_answers', 'good_answers', 'minimal_answers', 'answer_id', 'answer_text', 'accuracy']\n",
    "    df = pd.DataFrame(data=data, columns=columns)\n",
    "    return df\n",
    "\n",
    "    \n",
    "def process_dir(path : Path, dataset, write_filepath):\n",
    "    if dataset.lower() not in ['scientsbank', 'beetle']:\n",
    "        raise ValueError(f'\\\"{dataset}\\\" is not a valid value for \\'dataset\\' input. Use either \\\"sciEntsBank\\\" or \\\"beetle\\\".')\n",
    "    files = os.listdir(path)\n",
    "    dfs = {}\n",
    "    for file in files:\n",
    "        dfs[file] = process_xml(path / file, dataset)\n",
    "    \n",
    "\n",
    "    joined = pd.concat(dfs.values(), ignore_index=True)\n",
    "    return joined\n",
    "\n",
    "def get_prompt(df : pd.DataFrame, ways : int, dataset : str, model : str, file_name : str | Path, tune : bool = False, student_A_count : int = 1, student_B_count : int =0, student_C_count : int = 0, best_ref_count : int = 0, good_ref_count : int = 0, minimal_ref_count : int = 0):\n",
    "    ''''\n",
    "    get_rows - converts a DataFrame into a format for OpenAI finetuning API, writes to jsonl\n",
    "    ----------\n",
    "    Parameters\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe to be used\n",
    "    ways : int\n",
    "        The number of possible outputs for grading (MUST BE 2 OR 3)\n",
    "    dataset : str\n",
    "        Which dataset is being used (MUST BE \\\"scientsbank\\\" or \\\"beetle\\\")\n",
    "    model : str\n",
    "        Which model is being used (MUST BE \\\"gpt-3.5\\\", or \\\"gpt4\\\", or \\\"davinci-003\\\")\n",
    "    filename : str or pathlib.Path\n",
    "        The path/filename to write the final file to\n",
    "    tune : bool\n",
    "        Optional - True if this is to fine-tune, false otherwise. Default is false. Controls where or not there is response from the model. \n",
    "    student_A_count : int\n",
    "        Optional - How many of the correct student answers to use. min==1. - -1 means the max possible.\n",
    "    student_B_count : int\n",
    "        Optional - How many of the incorrect student answers to use. -1 means the max possible.\n",
    "    student_C_count : int\n",
    "        Optional - How many of the contradictory student answers to use. -1 means the max possible.\n",
    "    best_ref_count : int\n",
    "        Optional - How many of the best reference answers to use. \n",
    "    good_ref_count : int\n",
    "        Optional - How many of the good reference answers to use\n",
    "    minimal_ref_count : int\n",
    "        Optional - How many of the minimal reference answers to use\n",
    "\n",
    "    NOTES:\n",
    "    - student_A, student_B, student_C counts and best_ref, good_ref, and minimal_ref counts, -1 means the max\n",
    "    \n",
    "    '''\n",
    "    dataset = dataset.lower()\n",
    "    model = model.lower()\n",
    "\n",
    "    if dataset not in ['scientsbank', 'beetle']:\n",
    "        raise ValueError(f'\\\"{dataset}\\\" is not a valid value for \\'dataset\\' input. Use either \\\"sciEntsBank\\\" or \\\"beetle\\\".')\n",
    "    if ways not in [2,3]:\n",
    "        raise ValueError(f'\\'{str(ways)}\\' is not a 2-way or a 3-way classification. Use either the integers 2 or 3.')\n",
    "    if model not in ['gpt-3.5', 'gpt4', 'davinci-003']:\n",
    "        raise ValueError(f'\\'{model}\\' is not a valid model. Use \\\"gpt-3.5\\\", \\\"gpt4\\\", or \\\"davinci-003\\\".')\n",
    "    \n",
    "    questions = list(set(df['question_id']))\n",
    "\n",
    "    checker = lambda a : 1 if any(a) else 0\n",
    "\n",
    "    grade_scale_dict = {\n",
    "        2 : 'correct or incorrect',\n",
    "        3 : 'correct, incorrect, or contradictory; contradictory only in the case that the answer contradicts the provided correct answers'\n",
    "    }\n",
    "\n",
    "    grade_sample_dict = {\n",
    "        2 : 'correct or incorrect',\n",
    "        3 : 'correct, incorrect, or contradictory'\n",
    "    }\n",
    "\n",
    "    count_dict = {\n",
    "        2 : [student_A_count, student_B_count],\n",
    "        3 : [student_A_count, student_B_count, student_C_count]\n",
    "    }\n",
    "\n",
    "    role = 'This asistant is a chatbot designed to assess students\\' short answer responses on an exam.'\n",
    "\n",
    "    messages = []\n",
    "    data = []\n",
    "    \n",
    "\n",
    "    for j,question in enumerate(questions):\n",
    "        answers = df[df['question_id'] == question]\n",
    "\n",
    "        best_ref_count = min(best_ref_count, checker(answers['best_answers']),  len(answers['best_answers'].iloc[0]))\n",
    "        good_ref_count = min(good_ref_count, checker(answers['good_answers']), len(answers['good_answers'].iloc[0]))\n",
    "        minimal_ref_count = min(minimal_ref_count, checker(answers['minimal_answers']), len(answers['minimal_answers'].iloc[0]))\n",
    "\n",
    "        best_answers = answers['best_answers'].iloc[0][:best_ref_count]\n",
    "        good_answers = answers['good_answers'].iloc[0][:good_ref_count]\n",
    "        min_answers = answers['minimal_answers'].iloc[0][:minimal_ref_count]\n",
    "\n",
    "        module_check = lambda module, dataset: f' in {module}' if dataset == 'beetle' else ''\n",
    "\n",
    "        any_check = lambda ans: ans if any(ans) else ''\n",
    "        any_sep_check = lambda check, rl: rl if any(check) else ''\n",
    "\n",
    "        best_answer_str = [f' Best_answer_{str(i+1)} - \"{str(ans[\"#text\"])}, \"' for i,ans in enumerate(best_answers)]\n",
    "        best_str = [f' BEST - what the optimal answer would look like: '] + best_answer_str + ['.']\n",
    "        best_str = any_sep_check(best_answer_str, best_str)\n",
    "        best_str = ''.join(best_str)\n",
    "\n",
    "        good_answer_str = [f'Good_answer_{str(i+1)} - \"{ans[\"#text\"]}, \"' for i,ans in enumerate(good_answers)]\n",
    "        good_str = [f' GOOD - a sufficient answer: '] + good_answer_str + ['.']\n",
    "        good_str = any_sep_check(good_answer_str, good_str)\n",
    "        good_str = ''.join(good_str)\n",
    "\n",
    "        minimal_answer_str = [f'Minimal_answer_{str(i+1)} - \"{ans[\"#text\"]}, \"' for i,ans in enumerate(min_answers)]\n",
    "        minimal_str = [f' Minimal - answers that are not correct: '] + minimal_answer_str + ['.']\n",
    "        minimal_str = any_sep_check(minimal_answer_str, minimal_str)\n",
    "        minimal_str = ''.join(minimal_str)\n",
    "\n",
    "\n",
    "        student_A_ans = answers[answers['accuracy'] == 'correct']\n",
    "        student_B_ans = answers[answers['accuracy'] == 'incorrect']\n",
    "        student_C_ans = answers[answers['accuracy'] == 'contradictory']\n",
    "\n",
    "\n",
    "        student_A_count = len(student_A_ans) if student_A_count == -1 else student_A_count\n",
    "        student_B_count = len(student_B_ans) if student_B_count == -1 else student_B_count\n",
    "        student_C_count = len(student_C_ans) if student_C_count == -1 else student_C_count\n",
    "        \n",
    "\n",
    "        student_A_ans = student_A_ans[:student_A_count]\n",
    "        student_B_ans = student_B_ans[:student_B_count]\n",
    "        student_C_ans = student_C_ans[:student_C_count]\n",
    "\n",
    "        final_sans = pd.concat([student_A_ans, student_B_ans, student_C_ans]).sample(frac=1).reset_index()\n",
    "\n",
    "\n",
    "        final_ans_li = []\n",
    "\n",
    "        for i,ans in final_sans.iterrows(): \n",
    "            final_ans_li.append(f'student_answer_{str(i+1)} - \"{ans[\"answer_text\"]}\"')\n",
    "\n",
    "\n",
    "        final_ans_li.append('.')\n",
    "\n",
    "        final_ans_str = ', '.join(final_ans_li)\n",
    "\n",
    "        final_sam_li = []\n",
    "\n",
    "        for i,ans in final_sans.iterrows(): \n",
    "            final_sam_li.append(f'student_answer_{str(i+1)} - \"{grade_sample_dict[ways]}\"')\n",
    "        final_sam_li.append('.')\n",
    "\n",
    "        final_sam_str = ', '.join(final_sam_li)\n",
    "\n",
    "        final_sol_li = []\n",
    "\n",
    "        for i,ans in final_sans.iterrows(): \n",
    "            final_sol_li.append(f'student_answer_{str(i+1)} - \"{ans[\"accuracy\"]}\"')\n",
    "        final_sol_li.append('.')\n",
    "\n",
    "        final_sol_str = ', '.join(final_sol_li)\n",
    "\n",
    "        beginning_text = 'Suppose you are an educator, specifically, a K-12 teacher, focusing in science.'\n",
    "        module_text = f' You are grading an exam which aims to assess students\\' understanding{module_check(answers[\"module\"].iloc[0], dataset)}.'\n",
    "        questionText = f' This is the question they have been asked: \"{answers[\"question_text\"].iloc[0]}\". You should assess the student responses on the following scale: {grade_scale_dict[ways]}.'\n",
    "        ref_intro_text = f' You can gain a better understanding of the task through the following reference responses.'\n",
    "        ref_mid_text = f' They are classified in the following {any(best_answers) + any(good_answers) + any(min_answers)} category(s): '\n",
    "        ref_cat_list = [\"BEST\" if any(best_answers) else \"\", \"GOOD\" if any(good_answers) else \"\", \"MINIMAL\" if any(min_answers) else \"\"]\n",
    "        ref_cat_list = list(filter(None, ref_cat_list))\n",
    "        ref_cat_text = ' ,'.join(ref_cat_list)\n",
    "        ref_end_text = f' {ref_cat_text}.{best_str}{good_str}{minimal_str}'\n",
    "        task_intro_text = f' Based on these reference answers, could you grade the following {sum(count_dict[ways])} student responses.'\n",
    "        task_mid_text = f' Each number represents a different student\\'s response to the same question: {final_ans_str}'\n",
    "        task_end_text = f' Please respond in the following format: {final_sam_li}'\n",
    "        \n",
    "        prompt_text = beginning_text + module_text + questionText + ref_intro_text + ref_mid_text + ref_end_text + task_intro_text + task_mid_text + task_end_text\n",
    "        answer_text = f'Sure! Here are the grades that these students recieved: {final_sol_str}.'\n",
    "\n",
    "        prompt_text = prompt_text.replace(r'\\\\', '', -1); answer_text = answer_text.replace(r'\\\"', '', -1)\n",
    "        if model == 'davinci-003':\n",
    "            if tune:\n",
    "                messages.append({{\"prompt\" : prompt_text, \"completion\" : answer_text}})\n",
    "            else: \n",
    "                messages.append({{\"prompt\" : prompt_text}})\n",
    "\n",
    "        else:\n",
    "            if tune:\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"messages\" : [\n",
    "                            {\"role\" : \"system\", \"content\" : role},\n",
    "                            {\"role\" : \"user\", \"content\" : prompt_text},\n",
    "                            {\"role\" : \"assistant\", \"content\" : answer_text}\n",
    "                        ]\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"messages\" : [\n",
    "                            {\"role\" : \"system\", \"content\" : role},\n",
    "                            {\"role\" : \"user\", \"content\" : prompt_text},\n",
    "                        ]\n",
    "                    }\n",
    "                )\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    # I should probably make a system for storing objects (the dicts) so they aren't volatile - its fucking annoying and I can't recover the model responses\n",
    "    \n",
    "    return messages\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames:\n",
    "1. Dataset one\n",
    "2. Prompts + responses and everything\n",
    "3. Non-volatile responses + identifiers\n",
    "\n",
    "Need to code:\n",
    "- Add DF #2 and #3\n",
    "- Create storing code \n",
    "- Create function for recovering model responses from csv and into DF #2 and DF #3\n",
    "- Create function for getting response from GPT over prompt df\n",
    "    - Asyn\n",
    "- Repetitive storing per iteration of loop (make sure no bad overwriting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nclass DataSet():\\n    def __init__(self, train : bool, dataset : str, ways : int, path : str | Path):\\n        self.train = train\\n        self.dataset = dataset\\n        self.ways = ways\\n        self.path = path\\n\\n\\nsemeval = {\\n    'test' : {\\n        '2beetle' : {\\n            'path' : semeval3_path / 'test'\\n            'data' : \\n            'prompt' : \\n        },\\n        '3beetle' : {\\n            'path' : \\n            'data' : \\n            'prompt' : \\n        },\\n        '2scientsbank' : {\\n            'path' : \\n            'data' : \\n            'prompt' : \\n        },\\n        '3scientsbank' : {\\n            'path' : \\n            'data' : \\n            'prompt' : \\n        },\\n    },\\n    'train' : {\\n        '2beetle' : {\\n            'path' : \\n            'data' : \\n            'prompt' : \\n        },\\n        '3beetle' : {\\n            'path' : \\n            'data' : \\n            'prompt' : \\n        },\\n        '2scientsbank' : {\\n            'path' : \\n            'data' : \\n            'prompt' : \\n        },\\n        '3scientsbank' : {\\n            'path' : \\n            'data' : \\n            'prompt' : \\n        },\\n    },\\n}\\n\\n\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = Path(os.path.abspath('../'))\n",
    "\n",
    "dataset_path = base_path / 'datasets' \n",
    "\n",
    "semeval_path = dataset_path / 'cleaning' / 'SemEval-2013-task7'\n",
    "\n",
    "semeval3_path = dataset_path / 'semeval-2013-task7' / 'semeval-3way'\n",
    "\n",
    "training_path = semeval3_path / 'training'\n",
    "\n",
    "scients2way_path = training_path / '2way' / 'sciEntsbank'\n",
    "beetle2way_path = training_path / '2way' / 'beetle'\n",
    "scients3way_path = training_path / '3way' / 'sciEntsbank'\n",
    "beetle3way_path = training_path / '3way' / 'beetle'\n",
    "two_scientsbank = process_dir(scients2way_path, 'scientsbank', 'training_2way_scientsbank.csv')\n",
    "two_beetle = process_dir(beetle2way_path, 'beetle', 'training_2way_beetle.csv')\n",
    "three_scientsbank = process_dir(scients3way_path, 'scientsbank', 'training_3way_scientsbank.csv')\n",
    "three_beetle = process_dir(beetle3way_path, 'beetle', 'training_3way_beetle.csv')\n",
    "\n",
    "\n",
    "'''\n",
    "two_scientsbank = get_prompt(two_scientsbank, 2, 'scientsbank', 'gpt-3.5', True, 3, 3, 0, 1)\n",
    "two_beetle = get_prompt(two_beetle, 2, 'beetle', 'gpt-3.5', True, 3, 3, 0, 2, 1, 2)\n",
    "three_scientsbank = get_prompt(three_scientsbank, 2, 'scientsbank', 'gpt-3.5', True, 3, 3, 2, 1)\n",
    "three_beetle = get_prompt(three_beetle, 2, 'beetle', 'gpt-3.5', True, 3, 3, 2, 2, 1, 2)\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self, train : bool, dataset : str, ways : int, path : str | Path):\n",
    "        self.train = train\n",
    "        self.dataset = dataset\n",
    "        self.ways = ways\n",
    "        self.path = path\n",
    "\n",
    "\n",
    "semeval = {\n",
    "    'test' : {\n",
    "        '2beetle' : {\n",
    "            'path' : semeval3_path / 'test'\n",
    "            'data' : \n",
    "            'prompt' : \n",
    "        },\n",
    "        '3beetle' : {\n",
    "            'path' : \n",
    "            'data' : \n",
    "            'prompt' : \n",
    "        },\n",
    "        '2scientsbank' : {\n",
    "            'path' : \n",
    "            'data' : \n",
    "            'prompt' : \n",
    "        },\n",
    "        '3scientsbank' : {\n",
    "            'path' : \n",
    "            'data' : \n",
    "            'prompt' : \n",
    "        },\n",
    "    },\n",
    "    'train' : {\n",
    "        '2beetle' : {\n",
    "            'path' : \n",
    "            'data' : \n",
    "            'prompt' : \n",
    "        },\n",
    "        '3beetle' : {\n",
    "            'path' : \n",
    "            'data' : \n",
    "            'prompt' : \n",
    "        },\n",
    "        '2scientsbank' : {\n",
    "            'path' : \n",
    "            'data' : \n",
    "            'prompt' : \n",
    "        },\n",
    "        '3scientsbank' : {\n",
    "            'path' : \n",
    "            'data' : \n",
    "            'prompt' : \n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question_text</th>\n",
       "      <th>module</th>\n",
       "      <th>best_answers</th>\n",
       "      <th>good_answers</th>\n",
       "      <th>minimal_answers</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ST_59</td>\n",
       "      <td>Elena has a male lizard that has lived for sev...</td>\n",
       "      <td>ST</td>\n",
       "      <td>[{'@id': 'ST_59-a1', '#text': 'Elena should in...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>ST.59.382.1</td>\n",
       "      <td>Elena should add shelter.</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ST_59</td>\n",
       "      <td>Elena has a male lizard that has lived for sev...</td>\n",
       "      <td>ST</td>\n",
       "      <td>[{'@id': 'ST_59-a1', '#text': 'Elena should in...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>ST.59.386.1</td>\n",
       "      <td>She needs shelter.</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ST_59</td>\n",
       "      <td>Elena has a male lizard that has lived for sev...</td>\n",
       "      <td>ST</td>\n",
       "      <td>[{'@id': 'ST_59-a1', '#text': 'Elena should in...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>ST.59.389.1</td>\n",
       "      <td>Elena has to put homes inside of the habitat.</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ST_59</td>\n",
       "      <td>Elena has a male lizard that has lived for sev...</td>\n",
       "      <td>ST</td>\n",
       "      <td>[{'@id': 'ST_59-a1', '#text': 'Elena should in...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>ST.59.393.1</td>\n",
       "      <td>She needs to add another house or shelter and ...</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ST_59</td>\n",
       "      <td>Elena has a male lizard that has lived for sev...</td>\n",
       "      <td>ST</td>\n",
       "      <td>[{'@id': 'ST_59-a1', '#text': 'Elena should in...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>ST.59.396.1</td>\n",
       "      <td>Elena should include another home.</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_id                                      question_text module  \\\n",
       "0       ST_59  Elena has a male lizard that has lived for sev...     ST   \n",
       "1       ST_59  Elena has a male lizard that has lived for sev...     ST   \n",
       "2       ST_59  Elena has a male lizard that has lived for sev...     ST   \n",
       "3       ST_59  Elena has a male lizard that has lived for sev...     ST   \n",
       "4       ST_59  Elena has a male lizard that has lived for sev...     ST   \n",
       "\n",
       "                                        best_answers good_answers  \\\n",
       "0  [{'@id': 'ST_59-a1', '#text': 'Elena should in...           []   \n",
       "1  [{'@id': 'ST_59-a1', '#text': 'Elena should in...           []   \n",
       "2  [{'@id': 'ST_59-a1', '#text': 'Elena should in...           []   \n",
       "3  [{'@id': 'ST_59-a1', '#text': 'Elena should in...           []   \n",
       "4  [{'@id': 'ST_59-a1', '#text': 'Elena should in...           []   \n",
       "\n",
       "  minimal_answers    answer_id  \\\n",
       "0              []  ST.59.382.1   \n",
       "1              []  ST.59.386.1   \n",
       "2              []  ST.59.389.1   \n",
       "3              []  ST.59.393.1   \n",
       "4              []  ST.59.396.1   \n",
       "\n",
       "                                         answer_text accuracy  \n",
       "0                          Elena should add shelter.  correct  \n",
       "1                                 She needs shelter.  correct  \n",
       "2      Elena has to put homes inside of the habitat.  correct  \n",
       "3  She needs to add another house or shelter and ...  correct  \n",
       "4                 Elena should include another home.  correct  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_scientsbank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': \"This asistant is a chatbot designed to assess students' short answer responses on an exam.\"},\n",
       "  {'role': 'user',\n",
       "   'content': 'Suppose you are an educator, specifically, a K-12 teacher, focusing in science. You are grading an exam which aims to assess students\\' understanding. This is the question they have been asked: \"A solution is a type of mixture. What makes it different from other mixtures?\". You should assess the student responses on the following scale: correct or incorrect. You can gain a better understanding of the task through the following reference responses. They are classified in the following 1 category(s):  BEST. BEST - what the optimal answer would look like:  Best_answer_1 - \"A solution is a mixture formed when a solid dissolves in a liquid., \". Based on these reference answers, could you grade the following 6 student responses. Each number represents a different student\\'s response to the same question: student_answer_1 - \"You can see through it.\", student_answer_2 - \"It is a clear mixture.\", student_answer_3 - \"It dissolves the solid into a liquid that is see through.\", student_answer_4 - \"It is one material that dissolves into the other. Making a clear mixture. Although it could be colored, it has to be see through.\", student_answer_5 - \"In a solution one of the materials dissolves and the mixture is see through.\", student_answer_6 - \"It has to be a fairly clear.\", . Please respond in the following format: [\\'student_answer_1 - \"correct or incorrect\"\\', \\'student_answer_2 - \"correct or incorrect\"\\', \\'student_answer_3 - \"correct or incorrect\"\\', \\'student_answer_4 - \"correct or incorrect\"\\', \\'student_answer_5 - \"correct or incorrect\"\\', \\'student_answer_6 - \"correct or incorrect\"\\', \\'.\\']'}]}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twoscients_prompts_test = get_prompt(two_scientsbank, 2, 'scientsbank', 'gpt-3.5', 'scientsbank2waytrain', False, 3, 3, 0, 1)\n",
    "\n",
    "twoscients_prompts_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Suppose you are an educator, specifically, a K-12 teacher, focusing in science. You are grading an exam which aims to assess students\\' understanding. This is the question they have been asked: \"A solution is a type of mixture. What makes it different from other mixtures?\". You should assess the student responses on the following scale: correct or incorrect. You can gain a better understanding of the task through the following reference responses. They are classified in the following 1 category(s):  BEST. BEST - what the optimal answer would look like:  Best_answer_1 - \"A solution is a mixture formed when a solid dissolves in a liquid., \". Based on these reference answers, could you grade the following 6 student responses. Each number represents a different student\\'s response to the same question: student_answer_820 - \"It is one material that dissolves into the other. Making a clear mixture. Although it could be colored, it has to be see through.\", student_answer_822 - \"It dissolves the solid into a liquid that is see through.\", student_answer_825 - \"In a solution one of the materials dissolves and the mixture is see through.\", student_answer_819 - \"You can see through it.\", student_answer_821 - \"It is a clear mixture.\", student_answer_823 - \"It has to be a fairly clear.\", . Please respond in the following format: [\\'student_answer_820 - \"correct or incorrect\"\\', \\'student_answer_822 - \"correct or incorrect\"\\', \\'student_answer_825 - \"correct or incorrect\"\\', \\'student_answer_819 - \"correct or incorrect\"\\', \\'student_answer_821 - \"correct or incorrect\"\\', \\'student_answer_823 - \"correct or incorrect\"\\', \\'.\\']'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twoscients_prompts_test[0]['messages'][1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model='gpt-4-1106-preview',\n",
    "    messages=twoscients_prompts_test[0]['messages']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the optimal answer provided and the nature of the question, the key aspects we're looking for in a student's response to determine if it's correct are:\n",
      "\n",
      "1. Mention of a solution being a mixture where one substance (solid) dissolves into another (typically a liquid).\n",
      "2. The characteristic of the solution being clear or see-through, although it may be colored.\n",
      "\n",
      "Here are the assessments for each student response:\n",
      "\n",
      "student_answer_820 - Correct. The response indicates that one material dissolves into another, creating a clear mixture, which captures the essence of a solution.\n",
      "\n",
      "student_answer_822 - Correct. The student points out that a solution involves a solid dissolving into a liquid and the result is see-through, aligning with the fundamental properties of a solution.\n",
      "\n",
      "student_answer_825 - Correct. This answer correctly states that a solution is a mixture where a material dissolves and the resulting mixture is see-through.\n",
      "\n",
      "student_answer_819 - Incorrect. While it is true that a solution is typically see-through, this answer lacks the crucial information that a solution is formed by one substance dissolving into another.\n",
      "\n",
      "student_answer_821 - Incorrect. The response identifies a solution as a clear mixture but does not mention the dissolving process, which is a key aspect of what makes a solution different from other mixtures.\n",
      "\n",
      "student_answer_823 - Incorrect. Simply stating that a solution \"has to be fairly clear\" does not sufficiently describe what makes a solution different from other types of mixtures. There is no mention of the dissolving process.\n",
      "\n",
      "In summary, student answers 820, 822, and 825 are correct because they mention both the dissolving process and the characteristic of being see-through (even if colored). Responses 819, 821, and 823 are incorrect because they do not mention the dissolving process that characterizes a solution.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASAG_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
