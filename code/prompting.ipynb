{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 11,
=======
   "execution_count": 59,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dataset_util' from '/Users/mustafakhan/Library/Mobile Documents/com~apple~CloudDocs/My Stuff/Tech Shtuff/Code/OSR/Project1-ASAG/code/dataset_util.py'>"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 11,
=======
     "execution_count": 59,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Big box store imports\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import jsonlines\n",
    "from pprint import pprint\n",
    "import tiktoken\n",
    "import openai\n",
    "import xmltodict as xmd\n",
    "from dotenv import load_dotenv\n",
<<<<<<< Updated upstream
    "\n",
    "load_dotenv()\n"
=======
    "import pickle\n",
    "import asyncio\n",
    "from random import randint\n",
    "import importlib\n",
    "import io\n",
    "import datetime\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Custom file imports\n",
    "import dataset_util\n",
    "import dataset\n",
    "\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(dataset_util)\n",
    "\n"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xml(path: Path, dataset: str):\n",
    "    '''\n",
    "    process_xml : Converts xml file from scientsbank or beetle dataset to a pandas DataFrame\n",
    "    ----------\n",
    "    Parameters\n",
    "    path : pathlib.Path\n",
    "        The path to the xml file which you want to convert\n",
    "    dataset : str\n",
    "        The dataset being processed - either \"scientsbank\" or \"beetle\" (otehrwise raises ValueError)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing the data from that xml file\n",
    "    '''\n",
    "\n",
    "    dataset = dataset.lower()\n",
    "\n",
    "    if dataset not in ['scientsbank', 'beetle']:\n",
    "        raise ValueError(f'\\\"{dataset}\\\" is not a valid value for \\'dataset\\' input. Use either \\\"sciEntsBank\\\" or \\\"beetle\\\".')\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.read()\n",
    "    master = xmd.parse(data)\n",
    "\n",
    "    q_id = master['question']['@id']; q_text = master['question']['questionText']; q_module =  master['question']['@module']\n",
    "\n",
    "    \n",
    "    reference_answers = master['question']['referenceAnswers']['referenceAnswer']\n",
    "\n",
    "    \n",
    "    reference_processed = [None, None, None]\n",
    "    if dataset == 'scientsbank':\n",
    "        best_answers = [reference_answers]\n",
    "        good_answers = []\n",
    "        minimal_answers = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    elif type(reference_answers) == dict:\n",
    "        best_answers = [reference_answers if reference_answers['@category'] == 'BEST' else []]\n",
    "        good_answers = [reference_answers if reference_answers['@category'] == 'GOOD' else []]\n",
    "        minimal_answers = [reference_answers if reference_answers['@category'] == 'MINIMAL' else []]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    elif type(reference_answers) in [set, list]:\n",
    "        best_answers = [a for a in reference_answers if a['@category'] == 'BEST']\n",
    "        good_answers = [a for a in reference_answers if a['@category'] == 'GOOD']\n",
    "        minimal_answers = [a for a in reference_answers if a['@category'] == 'MINIMAL']\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "  \n",
    "    reference_processed = [best_answers, good_answers, minimal_answers]\n",
    "    answers = master['question']['studentAnswers']['studentAnswer']\n",
    "    data = []\n",
    "    for answer in answers:\n",
    "        data.append(\n",
    "            [q_id, q_text, q_module] + reference_processed + [answer['@id'], answer['#text'], answer['@accuracy']]\n",
    "        )\n",
    "    \n",
    "    columns = ['question_id', 'question_text', 'module', 'best_answers', 'good_answers', 'minimal_answers', 'answer_id', 'answer_text', 'accuracy']\n",
    "    df = pd.DataFrame(data=data, columns=columns)\n",
    "    return df\n",
    "\n",
    "    \n",
    "def process_dir(path : Path, dataset, write_filepath):\n",
    "    if dataset.lower() not in ['scientsbank', 'beetle']:\n",
    "        raise ValueError(f'\\\"{dataset}\\\" is not a valid value for \\'dataset\\' input. Use either \\\"sciEntsBank\\\" or \\\"beetle\\\".')\n",
    "    files = os.listdir(path)\n",
    "    dfs = {}\n",
    "    for file in files:\n",
    "        dfs[file] = process_xml(path / file, dataset)\n",
    "    \n",
    "\n",
    "    joined = pd.concat(dfs.values(), ignore_index=True)\n",
    "    return joined\n",
    "\n",
    "def get_prompt(df : pd.DataFrame, ways : int, dataset : str, model : str, file_name : str | Path, tune : bool = False, student_A_count : int = 1, student_B_count : int =0, student_C_count : int = 0, best_ref_count : int = 0, good_ref_count : int = 0, minimal_ref_count : int = 0):\n",
    "    ''''\n",
    "    get_rows - converts a DataFrame into a format for OpenAI finetuning API, writes to jsonl\n",
    "    ----------\n",
    "    Parameters\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe to be used\n",
    "    ways : int\n",
    "        The number of possible outputs for grading (MUST BE 2 OR 3)\n",
    "    dataset : str\n",
    "        Which dataset is being used (MUST BE \\\"scientsbank\\\" or \\\"beetle\\\")\n",
    "    model : str\n",
    "        Which model is being used (MUST BE \\\"gpt-3.5\\\", or \\\"gpt4\\\", or \\\"davinci-003\\\")\n",
    "    filename : str or pathlib.Path\n",
    "        The path/filename to write the final file to\n",
    "    tune : bool\n",
    "        Optional - True if this is to fine-tune, false otherwise. Default is false. Controls where or not there is response from the model. \n",
    "    student_A_count : int\n",
    "        Optional - How many of the correct student answers to use. min==1. - -1 means the max possible.\n",
    "    student_B_count : int\n",
    "        Optional - How many of the incorrect student answers to use. -1 means the max possible.\n",
    "    student_C_count : int\n",
    "        Optional - How many of the contradictory student answers to use. -1 means the max possible.\n",
    "    best_ref_count : int\n",
    "        Optional - How many of the best reference answers to use. \n",
    "    good_ref_count : int\n",
    "        Optional - How many of the good reference answers to use\n",
    "    minimal_ref_count : int\n",
    "        Optional - How many of the minimal reference answers to use\n",
    "\n",
    "    NOTES:\n",
    "    - student_A, student_B, student_C counts and best_ref, good_ref, and minimal_ref counts, -1 means the max\n",
    "    \n",
    "    '''\n",
    "    dataset = dataset.lower()\n",
    "    model = model.lower()\n",
    "\n",
    "    if dataset not in ['scientsbank', 'beetle']:\n",
    "        raise ValueError(f'\\\"{dataset}\\\" is not a valid value for \\'dataset\\' input. Use either \\\"sciEntsBank\\\" or \\\"beetle\\\".')\n",
    "    if ways not in [2,3]:\n",
    "        raise ValueError(f'\\'{str(ways)}\\' is not a 2-way or a 3-way classification. Use either the integers 2 or 3.')\n",
    "    if model not in ['gpt-3.5', 'gpt4', 'davinci-003']:\n",
    "        raise ValueError(f'\\'{model}\\' is not a valid model. Use \\\"gpt-3.5\\\", \\\"gpt4\\\", or \\\"davinci-003\\\".')\n",
    "    \n",
    "    questions = list(set(df['question_id']))\n",
    "\n",
    "    checker = lambda a : 1 if any(a) else 0\n",
    "\n",
    "    grade_scale_dict = {\n",
    "        2 : 'correct or incorrect',\n",
    "        3 : 'correct, incorrect, or contradictory; contradictory only in the case that the answer contradicts the provided correct answers'\n",
    "    }\n",
    "\n",
    "    grade_sample_dict = {\n",
    "        2 : 'correct or incorrect',\n",
    "        3 : 'correct, incorrect, or contradictory'\n",
    "    }\n",
    "\n",
    "    count_dict = {\n",
    "        2 : [student_A_count, student_B_count],\n",
    "        3 : [student_A_count, student_B_count, student_C_count]\n",
    "    }\n",
    "\n",
    "    role = 'This asistant is a chatbot designed to assess students\\' short answer responses on an exam.'\n",
    "\n",
    "    messages = []\n",
    "    data = []\n",
    "    \n",
    "\n",
    "    for j,question in enumerate(questions):\n",
    "        answers = df[df['question_id'] == question]\n",
    "\n",
    "        best_ref_count = min(best_ref_count, checker(answers['best_answers']),  len(answers['best_answers'].iloc[0]))\n",
    "        good_ref_count = min(good_ref_count, checker(answers['good_answers']), len(answers['good_answers'].iloc[0]))\n",
    "        minimal_ref_count = min(minimal_ref_count, checker(answers['minimal_answers']), len(answers['minimal_answers'].iloc[0]))\n",
    "\n",
    "        best_answers = answers['best_answers'].iloc[0][:best_ref_count]\n",
    "        good_answers = answers['good_answers'].iloc[0][:good_ref_count]\n",
    "        min_answers = answers['minimal_answers'].iloc[0][:minimal_ref_count]\n",
    "\n",
    "        module_check = lambda module, dataset: f' in {module}' if dataset == 'beetle' else ''\n",
    "\n",
    "        any_check = lambda ans: ans if any(ans) else ''\n",
    "        any_sep_check = lambda check, rl: rl if any(check) else ''\n",
    "\n",
    "        best_answer_str = [f' Best_answer_{str(i+1)} - \"{str(ans[\"#text\"])}, \"' for i,ans in enumerate(best_answers)]\n",
    "        best_str = [f' BEST - what the optimal answer would look like: '] + best_answer_str + ['.']\n",
    "        best_str = any_sep_check(best_answer_str, best_str)\n",
    "        best_str = ''.join(best_str)\n",
    "\n",
    "        good_answer_str = [f'Good_answer_{str(i+1)} - \"{ans[\"#text\"]}, \"' for i,ans in enumerate(good_answers)]\n",
    "        good_str = [f' GOOD - a sufficient answer: '] + good_answer_str + ['.']\n",
    "        good_str = any_sep_check(good_answer_str, good_str)\n",
    "        good_str = ''.join(good_str)\n",
    "\n",
    "        minimal_answer_str = [f'Minimal_answer_{str(i+1)} - \"{ans[\"#text\"]}, \"' for i,ans in enumerate(min_answers)]\n",
    "        minimal_str = [f' Minimal - answers that are not correct: '] + minimal_answer_str + ['.']\n",
    "        minimal_str = any_sep_check(minimal_answer_str, minimal_str)\n",
    "        minimal_str = ''.join(minimal_str)\n",
    "\n",
    "\n",
    "        student_A_ans = answers[answers['accuracy'] == 'correct']\n",
    "        student_B_ans = answers[answers['accuracy'] == 'incorrect']\n",
    "        student_C_ans = answers[answers['accuracy'] == 'contradictory']\n",
    "\n",
    "\n",
    "        student_A_count = len(student_A_ans) if student_A_count == -1 else student_A_count\n",
    "        student_B_count = len(student_B_ans) if student_B_count == -1 else student_B_count\n",
    "        student_C_count = len(student_C_ans) if student_C_count == -1 else student_C_count\n",
    "        \n",
    "\n",
    "        student_A_ans = student_A_ans[:student_A_count]\n",
    "        student_B_ans = student_B_ans[:student_B_count]\n",
    "        student_C_ans = student_C_ans[:student_C_count]\n",
    "\n",
    "        final_sans = pd.concat([student_A_ans, student_B_ans, student_C_ans]).sample(frac=1).reset_index()\n",
    "\n",
    "\n",
    "        final_ans_li = []\n",
    "\n",
    "        for i,ans in final_sans.iterrows(): \n",
    "            final_ans_li.append(f'student_answer_{str(i+1)} - \"{ans[\"answer_text\"]}\"')\n",
    "\n",
    "\n",
    "        final_ans_li.append('.')\n",
    "\n",
    "        final_ans_str = ', '.join(final_ans_li)\n",
    "\n",
    "        final_sam_li = []\n",
    "\n",
    "        for i,ans in final_sans.iterrows(): \n",
    "            final_sam_li.append(f'student_answer_{str(i+1)} - \"{grade_sample_dict[ways]}\"')\n",
    "        final_sam_li.append('.')\n",
    "\n",
    "        final_sam_str = ', '.join(final_sam_li)\n",
    "\n",
    "        final_sol_li = []\n",
    "\n",
    "        for i,ans in final_sans.iterrows(): \n",
    "            final_sol_li.append(f'student_answer_{str(i+1)} - \"{ans[\"accuracy\"]}\"')\n",
    "        final_sol_li.append('.')\n",
    "\n",
    "        final_sol_str = ', '.join(final_sol_li)\n",
    "\n",
    "        beginning_text = 'Suppose you are an educator, specifically, a K-12 teacher, focusing in science.'\n",
    "        module_text = f' You are grading an exam which aims to assess students\\' understanding{module_check(answers[\"module\"].iloc[0], dataset)}.'\n",
    "        questionText = f' This is the question they have been asked: \"{answers[\"question_text\"].iloc[0]}\". You should assess the student responses on the following scale: {grade_scale_dict[ways]}.'\n",
    "        ref_intro_text = f' You can gain a better understanding of the task through the following reference responses.'\n",
    "        ref_mid_text = f' They are classified in the following {any(best_answers) + any(good_answers) + any(min_answers)} category(s): '\n",
    "        ref_cat_list = [\"BEST\" if any(best_answers) else \"\", \"GOOD\" if any(good_answers) else \"\", \"MINIMAL\" if any(min_answers) else \"\"]\n",
    "        ref_cat_list = list(filter(None, ref_cat_list))\n",
    "        ref_cat_text = ' ,'.join(ref_cat_list)\n",
    "        ref_end_text = f' {ref_cat_text}.{best_str}{good_str}{minimal_str}'\n",
    "        task_intro_text = f' Based on these reference answers, could you grade the following {sum(count_dict[ways])} student responses.'\n",
    "        task_mid_text = f' Each number represents a different student\\'s response to the same question: {final_ans_str}'\n",
    "        task_end_text = f' Please respond in the following format: {final_sam_li}'\n",
    "        \n",
    "        prompt_text = beginning_text + module_text + questionText + ref_intro_text + ref_mid_text + ref_end_text + task_intro_text + task_mid_text + task_end_text\n",
    "        answer_text = f'Sure! Here are the grades that these students recieved: {final_sol_str}.'\n",
    "\n",
    "        prompt_text = prompt_text.replace(r'\\\\', '', -1); answer_text = answer_text.replace(r'\\\"', '', -1)\n",
    "        if model == 'davinci-003':\n",
    "            if tune:\n",
    "                messages.append({{\"prompt\" : prompt_text, \"completion\" : answer_text}})\n",
    "            else: \n",
    "                messages.append({{\"prompt\" : prompt_text}})\n",
    "\n",
    "        else:\n",
    "            if tune:\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"messages\" : [\n",
    "                            {\"role\" : \"system\", \"content\" : role},\n",
    "                            {\"role\" : \"user\", \"content\" : prompt_text},\n",
    "                            {\"role\" : \"assistant\", \"content\" : answer_text}\n",
    "                        ]\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"messages\" : [\n",
    "                            {\"role\" : \"system\", \"content\" : role},\n",
    "                            {\"role\" : \"user\", \"content\" : prompt_text},\n",
    "                        ]\n",
    "                    }\n",
    "                )\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    # I should probably make a system for storing objects (the dicts) so they aren't volatile - its fucking annoying and I can't recover the model responses\n",
    "    \n",
    "    return messages\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames:\n",
    "1. Dataset one\n",
    "2. Prompts + responses and everything\n",
    "3. Non-volatile responses + identifiers\n",
    "\n",
    "Need to code:\n",
    "- Add DF #2 and #3\n",
    "- Create storing code \n",
    "- Create function for recovering model responses from csv and into DF #2 and DF #3\n",
    "- Create function for getting response from GPT over prompt df\n",
    "    - Asyn\n",
    "- Repetitive storing per iteration of loop (make sure no bad overwriting)"
=======
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f1_multi_df(dfs : [pd.DataFrame], model : str, prompt_style : str):\n",
    "    ground_truths = [x for df in dfs for x in df.prompts['accuracy'].tolist()]\n",
    "    model_predicts = [x for df in dfs for x in df.prompts[f'{prompt_style}_{model}_answer'].tolist()]\n",
    "    return precision_recall_fscore_support(ground_truths, model_predicts, labels=['correct'], average='macro')[2]"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nclass DataSet():\\n    def __init__(self, train : bool, dataset : str, ways : int, path : str | Path):\\n        self.train = train\\n        self.dataset = dataset\\n        self.ways = ways\\n        self.path = path\\n\\n\\nsemeval = {\\n    'test' : {\\n        '2beetle' : {\\n            'path' : semeval3_path / 'test'\\n            'data' : \\n            'prompt' : \\n        },\\n        '3beetle' : {\\n            'path' : \\n            'data' : \\n            'prompt' : \\n        },\\n        '2scientsbank' : {\\n            'path' : \\n            'data' : \\n            'prompt' : \\n        },\\n        '3scientsbank' : {\\n            'path' : \\n            'data' : \\n            'prompt' : \\n        },\\n    },\\n    'train' : {\\n        '2beetle' : {\\n            'path' : \\n            'data' : \\n            'prompt' : \\n        },\\n        '3beetle' : {\\n            'path' : \\n            'data' : \\n            'prompt' : \\n        },\\n        '2scientsbank' : {\\n            'path' : \\n            'data' : \\n            'prompt' : \\n        },\\n        '3scientsbank' : {\\n            'path' : \\n            'data' : \\n            'prompt' : \\n        },\\n    },\\n}\\n\\n\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "base_path = Path(os.path.abspath('../'))\n",
    "dataset_path = base_path / 'datasets' \n",
    "semeval_path = dataset_path / 'cleaning' / 'SemEval-2013-task7'\n",
    "semeval3_path = dataset_path / 'semeval-2013-task7' / 'semeval-3way'\n",
    "training_path = semeval3_path / 'training'\n",
<<<<<<< Updated upstream
    "\n",
    "scients2way_path = training_path / '2way' / 'sciEntsbank'\n",
    "beetle2way_path = training_path / '2way' / 'beetle'\n",
    "scients3way_path = training_path / '3way' / 'sciEntsbank'\n",
    "beetle3way_path = training_path / '3way' / 'beetle'\n",
    "two_scientsbank = process_dir(scients2way_path, 'scientsbank', 'training_2way_scientsbank.csv')\n",
    "two_beetle = process_dir(beetle2way_path, 'beetle', 'training_2way_beetle.csv')\n",
    "three_scientsbank = process_dir(scients3way_path, 'scientsbank', 'training_3way_scientsbank.csv')\n",
    "three_beetle = process_dir(beetle3way_path, 'beetle', 'training_3way_beetle.csv')\n",
    "\n",
    "\n",
    "'''\n",
    "two_scientsbank = get_prompt(two_scientsbank, 2, 'scientsbank', 'gpt-3.5', True, 3, 3, 0, 1)\n",
    "two_beetle = get_prompt(two_beetle, 2, 'beetle', 'gpt-3.5', True, 3, 3, 0, 2, 1, 2)\n",
    "three_scientsbank = get_prompt(three_scientsbank, 2, 'scientsbank', 'gpt-3.5', True, 3, 3, 2, 1)\n",
    "three_beetle = get_prompt(three_beetle, 2, 'beetle', 'gpt-3.5', True, 3, 3, 2, 2, 1, 2)\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self, train : bool, dataset : str, ways : int, path : str | Path):\n",
    "        self.train = train\n",
    "        self.dataset = dataset\n",
    "        self.ways = ways\n",
    "        self.path = path\n",
    "\n",
    "\n",
    "semeval = {\n",
    "    'test' : {\n",
    "        '2beetle' : {\n",
    "            'path' : semeval3_path / 'test'\n",
    "            'data' : \n",
    "            'prompt' : \n",
    "        },\n",
    "        '3beetle' : {\n",
    "            'path' : \n",
    "            'data' : \n",
    "            'prompt' : \n",
    "        },\n",
    "        '2scientsbank' : {\n",
    "            'path' : \n",
    "            'data' : \n",
    "            'prompt' : \n",
    "        },\n",
    "        '3scientsbank' : {\n",
    "            'path' : \n",
    "            'data' : \n",
    "            'prompt' : \n",
    "        },\n",
    "    },\n",
    "    'train' : {\n",
    "        '2beetle' : {\n",
    "            'path' : \n",
    "            'data' : \n",
    "            'prompt' : \n",
    "        },\n",
    "        '3beetle' : {\n",
    "            'path' : \n",
    "            'data' : \n",
    "            'prompt' : \n",
    "        },\n",
    "        '2scientsbank' : {\n",
    "            'path' : \n",
    "            'data' : \n",
    "            'prompt' : \n",
    "        },\n",
    "        '3scientsbank' : {\n",
    "            'path' : \n",
    "            'data' : \n",
    "            'prompt' : \n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "'''\n"
=======
    "testing_path = semeval3_path / 'test'"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question_text</th>\n",
       "      <th>module</th>\n",
       "      <th>best_answers</th>\n",
       "      <th>good_answers</th>\n",
       "      <th>minimal_answers</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ST_59</td>\n",
       "      <td>Elena has a male lizard that has lived for sev...</td>\n",
       "      <td>ST</td>\n",
       "      <td>[{'@id': 'ST_59-a1', '#text': 'Elena should in...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>ST.59.382.1</td>\n",
       "      <td>Elena should add shelter.</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ST_59</td>\n",
       "      <td>Elena has a male lizard that has lived for sev...</td>\n",
       "      <td>ST</td>\n",
       "      <td>[{'@id': 'ST_59-a1', '#text': 'Elena should in...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>ST.59.386.1</td>\n",
       "      <td>She needs shelter.</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ST_59</td>\n",
       "      <td>Elena has a male lizard that has lived for sev...</td>\n",
       "      <td>ST</td>\n",
       "      <td>[{'@id': 'ST_59-a1', '#text': 'Elena should in...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>ST.59.389.1</td>\n",
       "      <td>Elena has to put homes inside of the habitat.</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ST_59</td>\n",
       "      <td>Elena has a male lizard that has lived for sev...</td>\n",
       "      <td>ST</td>\n",
       "      <td>[{'@id': 'ST_59-a1', '#text': 'Elena should in...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>ST.59.393.1</td>\n",
       "      <td>She needs to add another house or shelter and ...</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ST_59</td>\n",
       "      <td>Elena has a male lizard that has lived for sev...</td>\n",
       "      <td>ST</td>\n",
       "      <td>[{'@id': 'ST_59-a1', '#text': 'Elena should in...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>ST.59.396.1</td>\n",
       "      <td>Elena should include another home.</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_id                                      question_text module  \\\n",
       "0       ST_59  Elena has a male lizard that has lived for sev...     ST   \n",
       "1       ST_59  Elena has a male lizard that has lived for sev...     ST   \n",
       "2       ST_59  Elena has a male lizard that has lived for sev...     ST   \n",
       "3       ST_59  Elena has a male lizard that has lived for sev...     ST   \n",
       "4       ST_59  Elena has a male lizard that has lived for sev...     ST   \n",
       "\n",
       "                                        best_answers good_answers  \\\n",
       "0  [{'@id': 'ST_59-a1', '#text': 'Elena should in...           []   \n",
       "1  [{'@id': 'ST_59-a1', '#text': 'Elena should in...           []   \n",
       "2  [{'@id': 'ST_59-a1', '#text': 'Elena should in...           []   \n",
       "3  [{'@id': 'ST_59-a1', '#text': 'Elena should in...           []   \n",
       "4  [{'@id': 'ST_59-a1', '#text': 'Elena should in...           []   \n",
       "\n",
       "  minimal_answers    answer_id  \\\n",
       "0              []  ST.59.382.1   \n",
       "1              []  ST.59.386.1   \n",
       "2              []  ST.59.389.1   \n",
       "3              []  ST.59.393.1   \n",
       "4              []  ST.59.396.1   \n",
       "\n",
       "                                         answer_text accuracy  \n",
       "0                          Elena should add shelter.  correct  \n",
       "1                                 She needs shelter.  correct  \n",
       "2      Elena has to put homes inside of the habitat.  correct  \n",
       "3  She needs to add another house or shelter and ...  correct  \n",
       "4                 Elena should include another home.  correct  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_scientsbank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': \"This asistant is a chatbot designed to assess students' short answer responses on an exam.\"},\n",
       "  {'role': 'user',\n",
       "   'content': 'Suppose you are an educator, specifically, a K-12 teacher, focusing in science. You are grading an exam which aims to assess students\\' understanding. This is the question they have been asked: \"A solution is a type of mixture. What makes it different from other mixtures?\". You should assess the student responses on the following scale: correct or incorrect. You can gain a better understanding of the task through the following reference responses. They are classified in the following 1 category(s):  BEST. BEST - what the optimal answer would look like:  Best_answer_1 - \"A solution is a mixture formed when a solid dissolves in a liquid., \". Based on these reference answers, could you grade the following 6 student responses. Each number represents a different student\\'s response to the same question: student_answer_1 - \"You can see through it.\", student_answer_2 - \"It is a clear mixture.\", student_answer_3 - \"It dissolves the solid into a liquid that is see through.\", student_answer_4 - \"It is one material that dissolves into the other. Making a clear mixture. Although it could be colored, it has to be see through.\", student_answer_5 - \"In a solution one of the materials dissolves and the mixture is see through.\", student_answer_6 - \"It has to be a fairly clear.\", . Please respond in the following format: [\\'student_answer_1 - \"correct or incorrect\"\\', \\'student_answer_2 - \"correct or incorrect\"\\', \\'student_answer_3 - \"correct or incorrect\"\\', \\'student_answer_4 - \"correct or incorrect\"\\', \\'student_answer_5 - \"correct or incorrect\"\\', \\'student_answer_6 - \"correct or incorrect\"\\', \\'.\\']'}]}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twoscients_prompts_test = get_prompt(two_scientsbank, 2, 'scientsbank', 'gpt-3.5', 'scientsbank2waytrain', False, 3, 3, 0, 1)\n",
    "\n",
    "twoscients_prompts_test[0]"
=======
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "testing_sets = [\n",
    "    dataset.DataSet(testing_path / '2way' / 'beetle' / 'test-unseen-answers', False, 'beetle', 2, test_state='UA'), \n",
    "    dataset.DataSet(testing_path / '2way' / 'beetle' / 'test-unseen-questions', False, 'beetle', 2, test_state='UQ'), \n",
    "    dataset.DataSet(testing_path / '2way' / 'sciEntsBank' / 'test-unseen-answers', False, 'scientsbank', 2, test_state='UA'), \n",
    "    dataset.DataSet(testing_path / '2way' / 'sciEntsBank' / 'test-unseen-questions', False, 'scientsbank', 2, test_state='UQ'), \n",
    "    dataset.DataSet(testing_path / '2way' / 'sciEntsBank' / 'test-unseen-domains', False, 'scientsbank', 2, test_state='UD'), \n",
    "    dataset.DataSet(testing_path / '3way' / 'beetle' / 'test-unseen-answers', False, 'beetle', 3, test_state='UA'), \n",
    "    dataset.DataSet(testing_path / '3way' / 'beetle' / 'test-unseen-questions' , False, 'beetle', 3, test_state='UQ'), \n",
    "    dataset.DataSet(testing_path / '3way' / 'sciEntsBank' / 'test-unseen-answers', False, 'scientsbank', 3, test_state='UA'), \n",
    "    dataset.DataSet(testing_path / '3way' / 'sciEntsBank' / 'test-unseen-questions', False, 'scientsbank', 3, test_state='UQ'), \n",
    "    dataset.DataSet(testing_path / '3way' / 'sciEntsBank' / 'test-unseen-domains', False, 'scientsbank', 3, test_state='UD'), \n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_sets = [\n",
    "    dataset.DataSet(training_path / '2way' / 'beetle', True, 'beetle', 2),\n",
    "    dataset.DataSet(training_path / '2way' / 'sciEntsBank', True, 'sciEntsBank', 2),\n",
    "    dataset.DataSet(training_path / '3way' / 'beetle', True, 'beetle', 2),\n",
    "    dataset.DataSet(training_path / '3way' / 'sciEntsBank', True, 'sciEntsBank', 2),\n",
    "]\n",
    "\n",
    "\n",
    "tokens = []\n",
    "for i, setd in enumerate(testing_sets):\n",
    "    setd.make_prompts('gpt-3.5-turbo', 'kortemeyer', 3, 3, 2,2,1,2)\n",
    "    # tokens.append(setd.count_tokens('kortemeyer', 'gpt-3.5-turbo'))\n",
    "\n",
    "print((sum(tokens)))\n",
    "\n"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()\n"
=======
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.AsyncOpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Starting\n",
      "0 - Completed\n",
      "Sleeping after 0\n",
      "Awake from sleep after 0\n",
      "1 - Starting\n",
      "1 - Completed\n",
      "Sleeping after 1\n",
      "Awake from sleep after 1\n",
      "2 - Starting\n",
      "Rate limit reached, sleeping for a bit :)\n",
      "Up and awake\n",
      "2 - Completed\n",
      "Sleeping after 2\n",
      "Awake from sleep after 2\n",
      "3 - Starting\n",
      "3 - Completed\n",
      "Sleeping after 3\n",
      "Awake from sleep after 3\n",
      "4 - Starting\n",
      "4 - Completed\n",
      "Sleeping after 4\n",
      "Awake from sleep after 4\n",
      "5 - Starting\n",
      "5 - Completed\n",
      "Sleeping after 5\n",
      "Awake from sleep after 5\n",
      "6 - Starting\n",
      "6 - Completed\n",
      "Sleeping after 6\n",
      "Awake from sleep after 6\n",
      "7 - Starting\n",
      "Rate limit reached, sleeping for a bit :)\n",
      "Up and awake\n",
      "7 - Completed\n",
      "Sleeping after 7\n",
      "Awake from sleep after 7\n"
     ]
    }
   ],
   "source": [
    "for i, ds in enumerate(testing_sets[2:]):\n",
    "    print(f'{i} - Starting')\n",
    "    await ds.gpt_async(client, 'gpt-3.5-turbo-1106', 'kortemeyer', 70000)\n",
    "    print(f'{i} - Completed')\n",
    "    print(f'Sleeping after {i}')\n",
    "    await asyncio.sleep(65)\n",
    "    print(f'Awake from sleep after {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Starting\n",
      "0 - Completed\n",
      "Sleeping after 0\n",
      "Awake from sleep after 0\n",
      "1 - Starting\n",
      "1 - Completed\n",
      "Sleeping after 1\n",
      "Awake from sleep after 1\n",
      "2 - Starting\n",
      "Rate limit reached, sleeping for a bit :)\n",
      "Up and awake\n",
      "2 - Completed\n",
      "Sleeping after 2\n",
      "Awake from sleep after 2\n",
      "3 - Starting\n",
      "3 - Completed\n",
      "Sleeping after 3\n",
      "Awake from sleep after 3\n",
      "4 - Starting\n",
      "Rate limit reached, sleeping for a bit :)\n",
      "Up and awake\n",
      "Rate limit reached, sleeping for a bit :)\n",
      "Up and awake\n",
      "Rate limit reached, sleeping for a bit :)\n",
      "Up and awake\n",
      "4 - Completed\n",
      "Sleeping after 4\n",
      "Awake from sleep after 4\n",
      "5 - Starting\n",
      "5 - Completed\n",
      "Sleeping after 5\n",
      "Awake from sleep after 5\n",
      "6 - Starting\n",
      "6 - Completed\n",
      "Sleeping after 6\n",
      "Awake from sleep after 6\n",
      "7 - Starting\n",
      "Rate limit reached, sleeping for a bit :)\n",
      "Up and awake\n",
      "7 - Completed\n",
      "Sleeping after 7\n",
      "Awake from sleep after 7\n",
      "8 - Starting\n",
      "8 - Completed\n",
      "Sleeping after 8\n",
      "Awake from sleep after 8\n",
      "9 - Starting\n",
      "Rate limit reached, sleeping for a bit :)\n",
      "Up and awake\n",
      "Rate limit reached, sleeping for a bit :)\n",
      "Up and awake\n",
      "Rate limit reached, sleeping for a bit :)\n",
      "Up and awake\n",
      "9 - Completed\n",
      "Sleeping after 9\n",
      "Awake from sleep after 9\n"
     ]
    }
   ],
   "source": [
    "for i, ds in enumerate(testing_sets):\n",
    "    print(f'{i} - Starting')\n",
    "    await ds.gpt_async(client, 'gpt-4', 'kortemeyer', 35000)\n",
    "    print(f'{i} - Completed')\n",
    "    print(f'Sleeping after {i}')\n",
    "    await asyncio.sleep(65)\n",
    "    print(f'Awake from sleep after {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-22 19:37:48.271868: 0 - Starting\n",
      "2023-12-22 19:37:54.427153: 0 - Completed\n",
      "2023-12-22 19:37:54.427245: Sleeping after 0\n",
      "2023-12-22 19:38:59.428829: Awake from sleep after 0\n",
      "2023-12-22 19:38:59.429985: 1 - Starting\n",
      "2023-12-22 19:39:44.635762: 1 - Completed\n",
      "2023-12-22 19:39:44.636390: Sleeping after 1\n",
      "2023-12-22 19:40:49.638416: Awake from sleep after 1\n",
      "2023-12-22 19:40:49.639581: 2 - Starting\n",
      "2023-12-22 19:40:52.157553: 2 - Completed\n",
      "2023-12-22 19:40:52.157631: Sleeping after 2\n",
      "2023-12-22 19:41:57.159440: Awake from sleep after 2\n",
      "2023-12-22 19:41:57.161001: 3 - Starting\n",
      "2023-12-22 19:42:10.423803: 3 - Completed\n",
      "2023-12-22 19:42:10.424157: Sleeping after 3\n",
      "2023-12-22 19:43:15.425797: Awake from sleep after 3\n",
      "2023-12-22 19:43:15.426943: 4 - Starting\n",
      "Rate limit reached, sleeping for a bit :)\n",
      "Up and awake\n",
      "2023-12-22 19:44:42.050491: 4 - Completed\n",
      "2023-12-22 19:44:42.050601: Sleeping after 4\n",
      "2023-12-22 19:45:47.052922: Awake from sleep after 4\n",
      "2023-12-22 19:45:47.054826: 5 - Starting\n",
      "2023-12-22 19:45:52.718296: 5 - Completed\n",
      "2023-12-22 19:45:52.718383: Sleeping after 5\n",
      "2023-12-22 19:46:57.720780: Awake from sleep after 5\n",
      "2023-12-22 19:46:57.722600: 6 - Starting\n",
      "2023-12-22 19:47:41.839473: 6 - Completed\n",
      "2023-12-22 19:47:41.840090: Sleeping after 6\n",
      "2023-12-22 19:48:46.842955: Awake from sleep after 6\n",
      "2023-12-22 19:48:46.844505: 7 - Starting\n",
      "2023-12-22 19:48:49.341256: 7 - Completed\n",
      "2023-12-22 19:48:49.341347: Sleeping after 7\n",
      "2023-12-22 19:49:54.350089: Awake from sleep after 7\n",
      "2023-12-22 19:49:54.351857: 8 - Starting\n",
      "2023-12-22 19:50:07.988536: 8 - Completed\n",
      "2023-12-22 19:50:07.988658: Sleeping after 8\n",
      "2023-12-22 19:51:12.993304: Awake from sleep after 8\n",
      "2023-12-22 19:51:12.994859: 9 - Starting\n",
      "Rate limit reached, sleeping for a bit :)\n",
      "Up and awake\n",
      "2023-12-22 19:52:39.034145: 9 - Completed\n",
      "2023-12-22 19:52:39.034317: Sleeping after 9\n",
      "2023-12-22 19:53:44.036096: Awake from sleep after 9\n"
     ]
    }
   ],
   "source": [
    "for i,ds in enumerate(testing_sets):\n",
    "    print(f'{datetime.datetime.now()}: {i} - Starting')\n",
    "    await ds.gpt_async(client, 'ft:gpt-3.5-turbo-1106:personal::8WHxYRBn', 'kortemeyer', 72000)\n",
    "    print(f'{datetime.datetime.now()}: {i} - Completed')\n",
    "    print(f'{datetime.datetime.now()}: Sleeping after {i}')\n",
    "    await asyncio.sleep(65)\n",
    "    print(f'{datetime.datetime.now()}: Awake from sleep after {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload datasets for GPT-4, GPT-3.5, and Finetuned\n",
    "\n",
    "for ds in testing_sets:\n",
    "    ds.load_processed('gpt-4', 'kortemeyer')\n",
    "    ds.load_processed('gpt-3.5-turbo-1106', 'kortemeyer')\n",
    "    ds.load_processed('ft:gpt-3.5-turbo-1106:personal::8WHxYRBn', 'kortemeyer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Cell\n",
    "for i, ds in enumerate(testing_sets):\n",
    "    for model in ['gpt-4', 'gpt-3.5-turbo-1106', 'ft:gpt-3.5-turbo-1106:personal::8WHxYRBn']:\n",
    "        values = ds.prompts[f'kortemeyer_{model}_answer'].value_counts()\n",
    "        if len(values) != ds.ways:\n",
    "            print(f'Dataset {i} on {model} had the following value counts:')\n",
    "            print(values)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [['SCIENTSBANK'] * 6 + ['BEETLE'] * 4, ['2-way'] * 3 + ['3-way'] * 3 + ['2-way'] * 2 + ['3-way'] * 2, ['UA', 'UQ', 'UD'] * 2 + ['UA', 'UQ'] * 2]\n",
    "inds = ['FT: GPT-3.5', 'GPT-3.5', 'GPT-4']\n",
    "\n",
    "\n",
    "\n",
    "order_testing_sets = testing_sets[2:5] + testing_sets[7:] + testing_sets[:2] + testing_sets[5:7]\n",
    "\n",
    "\n",
    "data = [\n",
    "    [ds.model_f1_score('ft:gpt-3.5-turbo-1106:personal::8WHxYRBn', 'kortemeyer') for ds in order_testing_sets],\n",
    "    [pd.NA, calc_f1_multi_df(testing_sets[2:5], 'gpt-3.5-turbo-1106', 'kortemeyer'), pd.NA, \n",
    "     pd.NA, calc_f1_multi_df(testing_sets[7:], 'gpt-3.5-turbo-1106', 'kortemeyer'), pd.NA, \n",
    "     pd.NA, calc_f1_multi_df(testing_sets[:2], 'gpt-3.5-turbo-1106', 'kortemeyer'), \n",
    "     pd.NA, calc_f1_multi_df(testing_sets[5:7], 'gpt-3.5-turbo-1106', 'kortemeyer'), \n",
    "    ],\n",
    "    [pd.NA, calc_f1_multi_df(testing_sets[2:5], 'gpt-4', 'kortemeyer'), pd.NA, \n",
    "     pd.NA, calc_f1_multi_df(testing_sets[7:], 'gpt-4', 'kortemeyer'), pd.NA, \n",
    "     pd.NA, calc_f1_multi_df(testing_sets[:2], 'gpt-4', 'kortemeyer'), \n",
    "     pd.NA, calc_f1_multi_df(testing_sets[5:7], 'gpt-4', 'kortemeyer'), \n",
    "    ],\n",
    "]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = pd.DataFrame(data=data, index=inds, columns=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"6\" halign=\"left\">SCIENTSBANK</th>\n",
       "      <th colspan=\"4\" halign=\"left\">BEETLE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">2-way</th>\n",
       "      <th colspan=\"3\" halign=\"left\">3-way</th>\n",
       "      <th colspan=\"2\" halign=\"left\">2-way</th>\n",
       "      <th colspan=\"2\" halign=\"left\">3-way</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>UA</th>\n",
       "      <th>UQ</th>\n",
       "      <th>UD</th>\n",
       "      <th>UA</th>\n",
       "      <th>UQ</th>\n",
       "      <th>UD</th>\n",
       "      <th>UA</th>\n",
       "      <th>UQ</th>\n",
       "      <th>UA</th>\n",
       "      <th>UQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FT: GPT-3.5</th>\n",
       "      <td>0.767635</td>\n",
       "      <td>0.719449</td>\n",
       "      <td>0.693629</td>\n",
       "      <td>0.765531</td>\n",
       "      <td>0.746544</td>\n",
       "      <td>0.722562</td>\n",
       "      <td>0.736318</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.646154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-3.5</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.663121</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.644678</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.561480</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.583514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-4</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.758691</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.742158</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.641170</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.677485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            SCIENTSBANK                                                    \\\n",
       "                  2-way                         3-way                       \n",
       "                     UA        UQ        UD        UA        UQ        UD   \n",
       "FT: GPT-3.5    0.767635  0.719449  0.693629  0.765531  0.746544  0.722562   \n",
       "GPT-3.5            <NA>  0.663121      <NA>      <NA>  0.644678      <NA>   \n",
       "GPT-4              <NA>  0.758691      <NA>      <NA>  0.742158      <NA>   \n",
       "\n",
       "               BEETLE                                \n",
       "                2-way               3-way            \n",
       "                   UA        UQ        UA        UQ  \n",
       "FT: GPT-3.5  0.736318  0.631579  0.730769  0.646154  \n",
       "GPT-3.5          <NA>  0.561480      <NA>  0.583514  \n",
       "GPT-4            <NA>  0.641170      <NA>  0.677485  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "styles=[\n",
    "    {'selector': 'th',\n",
    "    'props': [\n",
    "        ('border-style', 'solid'),\n",
    "        ('border-color', 'Red'),\n",
    "        ('horizontal-align','center')\n",
    "    ]\n",
    "    }]\n",
    "\n",
    "results.style.set_table_styles(styles)\n",
    "display(results)\n",
    "\n",
    "results.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnded = results.round(2)\n",
    "rnded.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7357958550683773\n"
     ]
    }
   ],
   "source": [
    "accuracies = [[], []]\n",
    "for ds in testing_sets:\n",
    "    counts = ds.prompts['kortemeyer_gpt-4_correct'].value_counts()\n",
    "    accuracies[0].append(counts[True]); accuracies[1].append(counts[False])\n",
    "\n",
    "\n",
    "print(sum(accuracies[0]) / (sum(accuracies[0]) + sum(accuracies[1])))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "completion = testing_sets[1].prompts.loc[testing_sets[1].prompts['question_id'] == 'HYBRID_BURNED_OUT_EXPLAIN_Q2'].iloc[0]['gpt-4_completion']\n",
    "df = pd.DataFrame(data = [li.split(',') for li in completion.choices[0].message.content.replace('\\\"', '').split('\\n')][1:], columns = ['answer_id', f'{\"gpt-4\"}_answer'])\n",
    "\n",
    "print(testing_sets[1].prompts.loc[testing_sets[1].prompts['question_id'] == 'HYBRID_BURNED_OUT_EXPLAIN_Q2'].shape[0])\n",
    "\n",
    "print(testing_sets[1].prompts.loc[testing_sets[1].prompts['question_id'] == 'HYBRID_BURNED_OUT_EXPLAIN_Q2'].iloc[0]['kortemeyer_prompt']['messages'][1]['content'])\n",
    "# print([li.split(',') for li in completion.choices[0].message.content.replace('\\\"', '').split('\\n')][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>gpt-4_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj3-l2.qa98</td>\n",
       "      <td>incorrect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj7-l2.qa101</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj8-l2.qa108</td>\n",
       "      <td>incorrect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj9-l2.qa103</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj10-l2.qa106</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbjb38-l2.qa92</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbjb39-l2.qa92</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbjb40-l2.qa92</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbjb41-l2.qa93</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbjb42-l2.qa92</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             answer_id  \\\n",
       "50   SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj3-l2.qa98     \n",
       "51   SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj7-l2.qa101    \n",
       "52   SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj8-l2.qa108    \n",
       "53   SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj9-l2.qa103    \n",
       "54   SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj10-l2.qa106   \n",
       "..                                                                 ...   \n",
       "121  SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbjb38-l2.qa92   \n",
       "122  SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbjb39-l2.qa92   \n",
       "123  SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbjb40-l2.qa92   \n",
       "124  SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbjb41-l2.qa93   \n",
       "125  SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbjb42-l2.qa92   \n",
       "\n",
       "    gpt-4_answer  \n",
       "50   incorrect    \n",
       "51   correct      \n",
       "52   incorrect    \n",
       "53   correct      \n",
       "54   correct      \n",
       "..       ...      \n",
       "121  NaN          \n",
       "122  NaN          \n",
       "123  NaN          \n",
       "124  NaN          \n",
       "125  NaN          \n",
       "\n",
       "[76 rows x 2 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_sets[1].prompts.loc[testing_sets[1].prompts['question_id'] == 'HYBRID_BURNED_OUT_EXPLAIN_Q2'][['answer_id', 'gpt-4_answer']]"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fj = await client.fine_tuning.jobs.retrieve(\"ftjob-sIcgzJC32xstkw63dM5QtSBX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ft:gpt-3.5-turbo-1106:personal::8WHxYRBn'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fj.fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "await testing_sets[7].gpt_async(client, 'gpt-3.5-turbo-1106', 'kortemeyer', 60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_msgs = []\n",
    "valid_msgs = []\n",
    "for tset in training_sets:\n",
    "    new_msgs = tset.tune_messages('kortemeyer')\n",
    "    train_msgs = train_msgs + new_msgs[len(new_msgs) // 10:]\n",
    "    valid_msgs = valid_msgs + new_msgs[:len(new_msgs) // 10]\n",
    "\n",
    "with jsonlines.open('semeval-kortemeyer-tuning-v1.jsonl', mode='w') as writer:\n",
    "        writer.write_all(train_msgs)\n",
    "with jsonlines.open('semeval-kortemeyer-valid-v1.jsonl', mode='w') as writer:\n",
    "        writer.write_all(valid_msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes abt kortemeyer fine tuning job \n",
    "# Used 16 prompts per set\n",
    "# Used all of the prompts\n",
    "# I think a total of 309528 tokens\n",
    "# 330 prompts for tuning\n",
    "# 34 prompts for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade the student answers to the question \"Explain your reasoning.\". The reference answer is given as \"If B burns out, then there is still a closed path with the battery that contains A and C.\". The student answers are listed below in the format ID:answer. SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj3-l2.qa98:because they will still be contained in a closed path with the battery. \n",
      "SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj7-l2.qa101:there is a closed path that contains the battery, bulb a, and bulb c. \n",
      "SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj8-l2.qa108:not in the same path. \n",
      "SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj9-l2.qa103:bulbs a and c will still be conained in closed paths with the battery.. \n",
      "SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj10-l2.qa106:bulbs a and c are on a closed path with the battery. \n",
      "SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj11-l2.qa104:bulb b does not create a gap in the path of bulb a and bulb c. \n",
      "SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj11-l2.qa105:bulb b does not create a gap in the path of bulbs a and c.. \n",
      "SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj11-l2.qa106:bulb a and bulb c will light up.. \n",
      "SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj12-l2.qa109:Bulbs C and A will still be on closed paths with the battery. \n",
      "SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj13-l2.qa112:bulb b is not in the same path of bulbs a and c. \n",
      "SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj14-l2.qa108:Because Bulb b is not on a closed path.. \n",
      "SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj15-l2.qa128:because i said so. \n",
      "SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj18-l2.qa100:bulbs a and c will still be in a closed path with the battery. \n",
      "SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj19-l2.qa125:they are in different paths. \n",
      "SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj22-l2.qa108:l. \n",
      "SwitchesBulbsParallel-HYBRID_BURNED_OUT_EXPLAIN_Q2.sbj23-l2.qa106:Bulb A, Bulb C, and the battery are in a closed path..\n"
     ]
    }
   ],
   "source": [
    "with open('semeval_storage/test/2way/beetle/UQ_gpt-3.5', 'rb') as reader:\n",
    "    hi = pickle.load(reader)\n",
    "\n",
    "print(hi.iloc[85]['kortemeyer_prompt']['messages'][1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sets[0].process_responses('gpt-3.5-turbo-1106')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gpt-3.5-turbo-1106_correct'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/ASAG_1/lib/python3.11/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gpt-3.5-turbo-1106_correct'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/mustafakhan/Library/Mobile Documents/com~apple~CloudDocs/My Stuff/Tech Shtuff/Code/OSR/Project1-ASAG/code/prompting.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mustafakhan/Library/Mobile%20Documents/com~apple~CloudDocs/My%20Stuff/Tech%20Shtuff/Code/OSR/Project1-ASAG/code/prompting.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m test \u001b[39min\u001b[39;00m testing_sets:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mustafakhan/Library/Mobile%20Documents/com~apple~CloudDocs/My%20Stuff/Tech%20Shtuff/Code/OSR/Project1-ASAG/code/prompting.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(test\u001b[39m.\u001b[39;49mprompts[\u001b[39m'\u001b[39;49m\u001b[39mgpt-3.5-turbo-1106_correct\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mvalue_counts())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ASAG_1/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ASAG_1/lib/python3.11/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gpt-3.5-turbo-1106_correct'"
     ]
    }
   ],
   "source": [
    "for test in testing_sets:\n",
    "    print(test.prompts['gpt-3.5-turbo-1106_correct'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_data-sets', 'wb') as writer:\n",
    "    pickle.dump(testing_sets[0].prompts, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_data-seter', 'wb') as writer:\n",
    "    pickle.dump(testing_sets[0].data, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model='gpt-4-1106-preview',\n",
<<<<<<< Updated upstream
    "    messages=twoscients_prompts_test[0]['messages']\n",
=======
    "    messages=testing_sets[0].prompts['prompt'].iloc[0]['messages']\n",
>>>>>>> Stashed changes
    ")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the optimal answer provided and the nature of the question, the key aspects we're looking for in a student's response to determine if it's correct are:\n",
      "\n",
      "1. Mention of a solution being a mixture where one substance (solid) dissolves into another (typically a liquid).\n",
      "2. The characteristic of the solution being clear or see-through, although it may be colored.\n",
      "\n",
      "Here are the assessments for each student response:\n",
      "\n",
      "student_answer_820 - Correct. The response indicates that one material dissolves into another, creating a clear mixture, which captures the essence of a solution.\n",
      "\n",
      "student_answer_822 - Correct. The student points out that a solution involves a solid dissolving into a liquid and the result is see-through, aligning with the fundamental properties of a solution.\n",
      "\n",
      "student_answer_825 - Correct. This answer correctly states that a solution is a mixture where a material dissolves and the resulting mixture is see-through.\n",
      "\n",
      "student_answer_819 - Incorrect. While it is true that a solution is typically see-through, this answer lacks the crucial information that a solution is formed by one substance dissolving into another.\n",
      "\n",
      "student_answer_821 - Incorrect. The response identifies a solution as a clear mixture but does not mention the dissolving process, which is a key aspect of what makes a solution different from other mixtures.\n",
      "\n",
      "student_answer_823 - Incorrect. Simply stating that a solution \"has to be fairly clear\" does not sufficiently describe what makes a solution different from other types of mixtures. There is no mention of the dissolving process.\n",
      "\n",
      "In summary, student answers 820, 822, and 825 are correct because they mention both the dissolving process and the characteristic of being see-through (even if colored). Responses 819, 821, and 823 are incorrect because they do not mention the dissolving process that characterizes a solution.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ft:gpt-3.5-turbo-1106:personal::8TlLoivH'"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job = client.fine_tuning.jobs.retrieve('ftjob-s7xYQqoqdwgSFYb7hKs1eYrt')\n",
    "\n",
    "job.fine_tuned_model"
   ]
>>>>>>> Stashed changes
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASAG_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
