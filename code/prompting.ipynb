{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import jsonlines\n",
    "from pprint import pprint\n",
    "import tiktoken\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import xmltodict as xmd\n",
    "import pprint as pp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xml(path: Path, dataset: str):\n",
    "    '''\n",
    "    process_xml : Converts xml file from scientsbank or beetle dataset to a pandas DataFrame\n",
    "    ----------\n",
    "    Parameters\n",
    "    path : pathlib.Path\n",
    "        The path to the xml file which you want to convert\n",
    "    dataset : str\n",
    "        The dataset being processed - either \"scientsbank\" or \"beetle\" (otehrwise raises ValueError)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing the data from that xml file\n",
    "    '''\n",
    "\n",
    "    dataset = dataset.lower()\n",
    "\n",
    "    if dataset not in ['scientsbank', 'beetle']:\n",
    "        raise ValueError(f'\\\"{dataset}\\\" is not a valid value for \\'dataset\\' input. Use either \\\"sciEntsBank\\\" or \\\"beetle\\\".')\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.read()\n",
    "    master = xmd.parse(data)\n",
    "\n",
    "    q_id = master['question']['@id']; q_text = master['question']['questionText']; q_module =  master['question']['@module']\n",
    "\n",
    "    \n",
    "    reference_answers = master['question']['referenceAnswers']['referenceAnswer']\n",
    "\n",
    "    \n",
    "    reference_processed = [None, None, None]\n",
    "    if dataset == 'scientsbank':\n",
    "        best_answers = [reference_answers]\n",
    "        good_answers = [None]\n",
    "        minimal_answers = [None]\n",
    "\n",
    "        reference_processed = [best_answers, good_answers, minimal_answers]\n",
    "\n",
    "\n",
    "\n",
    "    elif type(reference_answers) == dict:\n",
    "        best_answers = [reference_answers if reference_answers['@category'] == 'BEST' else [None]]\n",
    "        good_answers = [reference_answers if reference_answers['@category'] == 'GOOD' else [None]]\n",
    "        minimal_answers = [reference_answers if reference_answers['@category'] == 'MINIMAL' else [None]]\n",
    "\n",
    "        reference_processed = [best_answers, good_answers, minimal_answers]\n",
    "\n",
    "\n",
    "\n",
    "    elif type(reference_answers) in [set, list]:\n",
    "        best_answers = [a for a in reference_answers if a['@category'] == 'BEST']\n",
    "        good_answers = [a for a in reference_answers if a['@category'] == 'GOOD']\n",
    "        minimal_answers = [a for a in reference_answers if a['@category'] == 'MINIMAL']\n",
    "\n",
    "        reference_processed = [best_answers, good_answers, minimal_answers]\n",
    "\n",
    "    \n",
    "\n",
    "    answers = master['question']['studentAnswers']['studentAnswer']\n",
    "    data = []\n",
    "    for answer in answers:\n",
    "        data.append(\n",
    "            [q_id, q_text, q_module] + reference_processed + [answer['@id'], answer['#text'], answer['@accuracy']]\n",
    "        )\n",
    "    \n",
    "    columns = ['question_id', 'question_text', 'module', 'best_answers', 'good_answers', 'minimal_answers', 'answer_id', 'answer_text', 'accuracy']\n",
    "    df = pd.DataFrame(data=data, columns=columns)\n",
    "    return df\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dir(path : Path, dataset, write_filepath):\n",
    "    if dataset.lower() not in ['scientsbank', 'beetle']:\n",
    "        raise ValueError(f'\\\"{dataset}\\\" is not a valid value for \\'dataset\\' input. Use either \\\"sciEntsBank\\\" or \\\"beetle\\\".')\n",
    "    files = os.listdir(path)\n",
    "    dfs = {}\n",
    "    for file in files:\n",
    "        dfs[file] = process_xml(path / file, dataset)\n",
    "    \n",
    "\n",
    "    joined = pd.concat(dfs.values(), ignore_index=True)\n",
    "    return joined\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(os.path.abspath('../'))\n",
    "\n",
    "dataset_path = base_path / 'datasets' \n",
    "\n",
    "semeval_path = dataset_path / 'cleaning' / 'SemEval-2013-task7'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval3_path = dataset_path / 'semeval-2013-task7' / 'semeval-3way'\n",
    "\n",
    "training_path = semeval3_path / 'training'\n",
    "\n",
    "scients2way_path = training_path / '2way' / 'sciEntsbank'\n",
    "beetle2way_path = training_path / '2way' / 'beetle'\n",
    "scients3way_path = training_path / '3way' / 'sciEntsbank'\n",
    "beetle3way_path = training_path / '3way' / 'beetle'\n",
    "two_scientsbank = process_dir(scients2way_path, 'scientsbank', 'training_2way_scientsbank.csv')\n",
    "two_beetle = process_dir(beetle2way_path, 'beetle', 'training_2way_beetle.csv')\n",
    "three_scientsbank = process_dir(scients3way_path, 'scientsbank', 'training_3way_scientsbank.csv')\n",
    "three_beetle = process_dir(beetle3way_path, 'beetle', 'training_3way_beetle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'@id': 'ST_59-a1',\n",
       "  '#text': 'Elena should include a separate shelter for each lizard.'}]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_scientsbank['best_answers'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows(df : pd.DataFrame, ways : int, dataset : str, model : str, file_name : str | Path, student_A_count : int = 1, student_B_count : int =0, student_C_count : int = 0, best_ref_count : int = 0, good_ref_count : int = 0, minimal_ref_count : int = 0):\n",
    "    ''''\n",
    "    get_rows - converts a DataFrame into a format for OpenAI finetuning API, writes to jsonl\n",
    "    ----------\n",
    "    Parameters\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe to be used\n",
    "    ways : int\n",
    "        The number of possible outputs for grading (MUST BE 2 OR 3)\n",
    "    dataset : str\n",
    "        Which dataset is being used (MUST BE \\\"scientsbank\\\" or \\\"beetle\\\")\n",
    "    model : str\n",
    "        Which model is being used (MUST BE \\\"gpt-3.5\\\", or \\\"gpt4\\\", or \\\"davinci-003\\\")\n",
    "    filename : str or pathlib.Path\n",
    "        The path/filename to write the final file to\n",
    "    student_A_count : int\n",
    "        How many of the correct student answers to use. min==1. - -1 means the max possible.\n",
    "    student_B_count : int\n",
    "        How many of the incorrect student answers to use. -1 means the max possible.\n",
    "    student_C_count : int\n",
    "        How many of the contradictory student answers to use. -1 means the max possible.\n",
    "    best_ref_count : int\n",
    "        How many of the best reference answers to use. \n",
    "    good_ref_count : int\n",
    "        How many of the good reference answers to use\n",
    "    minimal_ref_count : int\n",
    "        How many of the minimal reference answers to use\n",
    "\n",
    "    NOTES:\n",
    "    - student_A, student_B, student_C counts and best_ref, good_ref, and minimal_ref counts, -1 means the max\n",
    "    \n",
    "    '''\n",
    "    dataset = dataset.lower()\n",
    "    model = model.lower()\n",
    "\n",
    "    if dataset not in ['scientsbank', 'beetle']:\n",
    "        raise ValueError(f'\\\"{dataset}\\\" is not a valid value for \\'dataset\\' input. Use either \\\"sciEntsBank\\\" or \\\"beetle\\\".')\n",
    "    if ways not in [2,3]:\n",
    "        raise ValueError(f'\\'{str(ways)}\\' is not a 2-way or a 3-way classification. Use either the integers 2 or 3.')\n",
    "    if model not in ['gpt-3.5', 'gpt4', 'davinci-003']:\n",
    "        raise ValueError(f'\\'{model}\\' is not a valid model. Use \\\"gpt-3.5\\\", \\\"gpt4\\\", or \\\"davinci-003\\\".')\n",
    "    \n",
    "    questions = list(set(df['question_id']))\n",
    "\n",
    "    checker = lambda a : 1 if any(a) else 0\n",
    "\n",
    "    grade_scale_dict = {\n",
    "        2 : 'correct or incorrect',\n",
    "        3 : 'correct, incorrect, or contradictory; contradictory only in the case that the answer contradicts the provided correct answers'\n",
    "    }\n",
    "\n",
    "    grade_sample_dict = {\n",
    "        2 : 'correct or incorrect',\n",
    "        3 : 'correct, incorrect, or contradictory'\n",
    "    }\n",
    "\n",
    "    count_dict = {\n",
    "        2 : [student_A_count, student_B_count],\n",
    "        3 : [student_A_count, student_B_count, student_C_count]\n",
    "    }\n",
    "\n",
    "    role = 'This asistant is a chatbot designed to assess students\\' short answer responses on an exam.'\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    for question in questions:\n",
    "        answers = df[df['question_id'] == question]\n",
    "\n",
    "        best_ref_count = min(best_ref_count, checker(answers['best_answers']),  len(answers['best_answers'].iloc[0]))\n",
    "        good_ref_count = min(good_ref_count, checker(answers['good_answers']), len(answers['good_answers'].iloc[0]))\n",
    "        minimal_ref_count = min(minimal_ref_count, checker(answers['minimal_answers']), len(answers['minimal_answers'].iloc[0]))\n",
    "\n",
    "        best_answers = answers['best_answers'].iloc[0][:best_ref_count]\n",
    "        good_answers = answers['good_answers'].iloc[0][:good_ref_count]\n",
    "        min_answers = answers['minimal_answers'].iloc[0][:minimal_ref_count]\n",
    "\n",
    "        module_check = lambda module, dataset: f' in {module}' if dataset == 'beetle' else ''\n",
    "\n",
    "        any_check = lambda ans: ans if any(ans) else ''\n",
    "        any_sep_check = lambda check, rl: rl if any(check) else ''\n",
    "\n",
    "        best_answer_str = [f' Best_answer_{str(i+1)} - \"{str(ans[\"#text\"])}, \"' for i,ans in enumerate(best_answers)]\n",
    "        best_str = [f' BEST - what the optimal answer would look like: '] + best_answer_str + ['.']\n",
    "        best_str = any_sep_check(best_answer_str, best_str)\n",
    "        best_str = ''.join(best_str)\n",
    "\n",
    "        good_answer_str = [f'Good_answer_{str(i+1)} - \"{ans[\"#text\"]}, \"' for i,ans in enumerate(good_answers)]\n",
    "        good_str = [f' GOOD - a sufficient answer: '] + good_answer_str + ['.']\n",
    "        good_str = any_sep_check(good_answer_str, good_str)\n",
    "        good_str = ''.join(good_str)\n",
    "\n",
    "        minimal_answer_str = [f'Minimal_answer_{str(i+1)} - \"{ans[\"#text\"]}, \"' for i,ans in enumerate(min_answers)]\n",
    "        minimal_str = [f' Minimal - answers that are not correct: '] + minimal_answer_str + ['.']\n",
    "        minimal_str = any_sep_check(minimal_answer_str, minimal_str)\n",
    "        minimal_str = ''.join(minimal_str)\n",
    "\n",
    "\n",
    "        student_A_ans = answers[answers['accuracy'] == 'correct']\n",
    "        student_B_ans = answers[answers['accuracy'] == 'incorrect']\n",
    "        student_C_ans = answers[answers['accuracy'] == 'contradictory']\n",
    "\n",
    "\n",
    "        student_A_count = len(student_A_ans) if student_A_count == -1 else student_A_count\n",
    "        student_B_count = len(student_B_ans) if student_B_count == -1 else student_B_count\n",
    "        student_C_count = len(student_C_ans) if student_C_count == -1 else student_C_count\n",
    "        \n",
    "\n",
    "        student_A_ans = student_A_ans[:student_A_count]\n",
    "        student_B_ans = student_B_ans[:student_B_count]\n",
    "        student_C_ans = student_C_ans[:student_C_count]\n",
    "\n",
    "        final_sans = pd.concat([student_A_ans, student_B_ans, student_C_ans])\n",
    "\n",
    "        final_ans_li = []\n",
    "\n",
    "        for i,ans in final_sans.iterrows(): \n",
    "            final_ans_li.append(f'student_answer_{str(i+1)} - \"{ans[\"answer_text\"]}\"')\n",
    "        final_ans_li.append('.')\n",
    "\n",
    "        final_ans_str = ', '.join(final_ans_li)\n",
    "\n",
    "        final_sam_li = []\n",
    "\n",
    "        for i,ans in final_sans.iterrows(): \n",
    "            final_sam_li.append(f'student_answer_{str(i+1)} - \"{grade_sample_dict[ways]}\"')\n",
    "        final_sam_li.append('.')\n",
    "\n",
    "        final_sam_str = ', '.join(final_sam_li)\n",
    "\n",
    "        final_sol_li = []\n",
    "\n",
    "        for i,ans in final_sans.iterrows(): \n",
    "            final_sol_li.append(f'student_answer_{str(i+1)} - \"{ans[\"accuracy\"]}\"')\n",
    "        final_sol_li.append('.')\n",
    "\n",
    "        final_sol_str = ', '.join(final_sol_li)\n",
    "\n",
    "        beginning_text = 'Suppose you are an educator, specifically, a K-12 teacher, focusing in science.'\n",
    "        module_text = f' You are grading an exam which aims to assess students\\' understanding{module_check(answers[\"module\"].iloc[0], dataset)}.'\n",
    "        questionText = f' This is the question they have been asked: \"{answers[\"question_text\"].iloc[0]}\". You should assess the student responses on the following scale: {grade_scale_dict[ways]}.'\n",
    "        ref_intro_text = f' You can gain a better understanding of the task through the following reference responses.'\n",
    "        ref_mid_text = f' They are classified in the following {any(best_answers) + any(good_answers) + any(min_answers)} category(s): '\n",
    "        ref_cat_list = [\"BEST\" if any(best_answers) else \"\", \"GOOD\" if any(good_answers) else \"\", \"MINIMAL\" if any(min_answers) else \"\"]\n",
    "        ref_cat_list = list(filter(None, ref_cat_list))\n",
    "        ref_cat_text = ' ,'.join(ref_cat_list)\n",
    "        ref_end_text = f' {ref_cat_text}.{best_str}{good_str}{minimal_str}.'\n",
    "        task_intro_text = f' Based on these reference answers, could you grade the following {sum(count_dict[ways])} student responses.'\n",
    "        task_mid_text = f' Each number represents a different student\\'s response to the same question: {final_ans_str}.'\n",
    "        task_end_text = f' Please respond in the following format: {final_sam_li}'\n",
    "        \n",
    "        prompt_text = beginning_text + module_text + questionText + ref_intro_text + ref_mid_text + ref_end_text + task_intro_text + task_mid_text\n",
    "        answer_text = f'Sure! Here are the grades that these students recieved: {final_sol_str}.'\n",
    "\n",
    "        prompt_text = prompt_text.replace(r'\\\\', '', -1); answer_text = answer_text.replace(r'\\\"', '', -1)\n",
    "        if model == 'davinci-003':\n",
    "        \n",
    "            messages.append(\n",
    "                {\n",
    "                    {\"prompt\" : prompt_text, \"completion\" : answer_text}\n",
    "                }\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            \n",
    "            messages.append(\n",
    "                {\n",
    "                    \"messages\" : [\n",
    "                        {\"role\" : \"system\", \"content\" : role},\n",
    "                        {\"role\" : \"user\", \"content\" : prompt_text},\n",
    "                        {\"role\" : \"assistant\", \"content\" : answer_text}\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "\n",
    "    with jsonlines.open('output1.jsonl', mode='w') as writer:\n",
    "        writer.write_all(messages)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoscients_msg = get_rows(two_scientsbank, 2, 'scientsbank', 'gpt-3.5', 'scientsbank2waytrain', 3, 3, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "print(len(encoding.encode(twoscients_msg[0]['messages'][0]['content']+ twoscients_msg[0]['messages'][1]['content'] + ' ' + twoscients_msg[0]['messages'][2]['content'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASAG_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
